{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2aeac6-7522-4690-ac27-5156ac10712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53da19ad-e6d1-41e5-a09d-e952ef738cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semEval as sem\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7d34b9-ce61-4c24-954e-686640bd8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json data into te \n",
    "data = sem.load_data()\n",
    "\n",
    "utt_emotions, cause_spans, utterances, speakers, emotions, dia_utt, all_vocab = sem.format_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0a5b0e-e42d-428d-8bc8-58d0b678d748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33fda5b-f4fc-4e76-8ad9-9df57aa57d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = sem.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d873e9-a3db-43cb-88a6-e3bcfa55ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>']\n",
    "tokenized_data = my_tokenizer(all_vocab)\n",
    "vocab = sorted(set([w for ws in tokenized_data + [SPECIAL_TOKENS] for w in ws]))\n",
    "\n",
    "with open('vocab.txt', 'w', encoding=\"utf-8\") as vf:\n",
    "    vf.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9610fead-6064-4cee-b2d5-f5339f864f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write tokenized data\n",
    "import csv \n",
    "\n",
    "with open('tokenized_data.csv', 'w', encoding = 'utf-8') as f:\n",
    "     \n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    \n",
    "    write.writerows(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a385367-2e90-4cda-87b2-caf680fa4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tokenized data\n",
    "with open('tokenized_data.csv', 'r', encoding = 'utf-8') as read_obj: \n",
    "  \n",
    "    # Return a reader object which will \n",
    "    # iterate over lines in the given csvfile \n",
    "    csv_reader = csv.reader(read_obj) \n",
    "  \n",
    "    # convert string to list \n",
    "    tokenized_data = list(csv_reader) \n",
    "    # the reader is reading in empyt lists. we only wnat lists that are full\n",
    "    tokenized_data = [sent for sent in tokenized_data if sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16f69256-64a4-42f1-b1c6-3ca0b315e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\menam\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# create imbeddings using skipgram\n",
    "model2 = gensim.models.Word2Vec(tokenized_data, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    "word_vectors = model2.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edde87-6c43-4043-9d6a-05aee6a1693c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd15fbdf-0e9a-435a-b17e-c0a36744a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "import numpy as np\n",
    "# Opening JSON file\n",
    "f = open('data/Subtask_1_1_train.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e121f5-70da-4dff-b2d9-84c629cbf170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "    def __init__(self, pad_symbol: Optional[str] = \"<PAD>\"):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<PAD>\".\n",
    "        \"\"\"\n",
    "        self.pad_symbol = pad_symbol\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    def __call__(self, batch: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes each sentence in the batch, and pads them if necessary so\n",
    "        that we have equal length sentences in the batch.\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "        Returns:\n",
    "            List[List[str]]: A List of equal-length token Lists.\n",
    "        \"\"\"\n",
    "        batch = self.tokenize(batch)\n",
    "        batch = self.pad(batch)\n",
    "        return batch\n",
    "\n",
    "    def tokenize(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes the List of string sentences into a Lists of tokens using spacy tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (List[str]): The input sentence.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The tokenized version of the sentence.\n",
    "        \"\"\"\n",
    "        tokened_sents = []\n",
    "        for sent in sentences:\n",
    "            tokened_sents.append(['<SOS>'] + [w.text for w in self.nlp(sent)] + ['<EOS>'])\n",
    "        return tokened_sents\n",
    "\n",
    "    def pad(self, batch: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"Appends pad symbols to each tokenized sentence in the batch such that\n",
    "        every List of tokens is the same length. This means that the max length sentence\n",
    "        will not be padded.\n",
    "\n",
    "        Args:\n",
    "            batch (List[List[str]]): Batch of tokenized sentences.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: Batch of padded tokenized sentences. \n",
    "        \"\"\"\n",
    "        max_len = len(max(batch, key=len))\n",
    "        for sent in batch:\n",
    "            pad_len = max_len - len(sent) \n",
    "            sent += ['<P>'] * pad_len\n",
    "        # TODO: For each sentence in the batch, append the special <P>\n",
    "        #       symbol to it n times to make all sentences equal length\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774363ed-96e4-40a5-8994-03be4b615829",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m----> 5\u001b[0m     matches \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mfindall(pattern,d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion_utterance_ID\u001b[39m\u001b[38;5;124m'\u001b[39m], flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m     dia, utt \u001b[38;5;241m=\u001b[39m matches[\u001b[38;5;241m0\u001b[39m], matches[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m     dia_utt\u001b[38;5;241m.\u001b[39mappend([dia,utt])\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bee3f4c7-c097-4a31-84a3-c942cb0f2638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '$', '%', '&', \"'s\", ',', '.', '...', '/']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer = Tokenizer()\n",
    "#print(utterances[0])\n",
    "tokenized_data = my_tokenizer.tokenize(all_vocab)\n",
    "\n",
    "\n",
    "\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>']\n",
    "\n",
    "vocab = sorted(set([w for ws in tokenized_data + [SPECIAL_TOKENS] for w in ws]))\n",
    "vocab[:10]\n",
    "with open('vocab.txt', 'w', encoding=\"utf-8\") as vf:\n",
    "    vf.write('\\n'.join(vocab))\n",
    "\n",
    "#for convo, label in zip(utterances[:5],utt_emotions[:5]):\n",
    " #   print(convo,label)\n",
    "\n",
    "#for convo, causes in zip(utterances[:5],cause_spans[:5]):\n",
    " #   print(convo, causes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39eca611-b1ac-44b8-9576-215f3689edac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS>',\n",
       " 'Alright',\n",
       " ',',\n",
       " 'so',\n",
       " 'I',\n",
       " 'am',\n",
       " 'back',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cafeteria',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'realize',\n",
       " 'I',\n",
       " 'am',\n",
       " 'totally',\n",
       " 'naked',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " ',',\n",
       " 'yeah',\n",
       " '.',\n",
       " 'Had',\n",
       " 'that',\n",
       " 'dream',\n",
       " '.',\n",
       " '<EOS>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for sent in tokenized_data:\n",
    "    for word in sent:\n",
    "        data.append(word)\n",
    "\n",
    "#\"\".join(tokenized_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6602440a-d226-437d-9062-9c12c596685c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'teacher' in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e6b997f-7941-47a6-bc92-1fe06c306d5b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS>',\n",
       " 'Alright',\n",
       " ',',\n",
       " 'so',\n",
       " 'I',\n",
       " 'am',\n",
       " 'back',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cafeteria',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'realize',\n",
       " 'I',\n",
       " 'am',\n",
       " 'totally',\n",
       " 'naked',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " ',',\n",
       " 'yeah',\n",
       " '.',\n",
       " 'Had',\n",
       " 'that',\n",
       " 'dream',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Alright',\n",
       " ',',\n",
       " 'so',\n",
       " 'I',\n",
       " 'am',\n",
       " 'back',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cafeteria',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'realize',\n",
       " 'I',\n",
       " 'am',\n",
       " 'totally',\n",
       " 'naked',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " ',',\n",
       " 'yeah',\n",
       " '.',\n",
       " 'Had',\n",
       " 'that',\n",
       " 'dream',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Then',\n",
       " 'I',\n",
       " 'look',\n",
       " 'down',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'realize',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'phone',\n",
       " '...',\n",
       " 'there',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Alright',\n",
       " ',',\n",
       " 'so',\n",
       " 'I',\n",
       " 'am',\n",
       " 'back',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cafeteria',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'realize',\n",
       " 'I',\n",
       " 'am',\n",
       " 'totally',\n",
       " 'naked',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " ',',\n",
       " 'yeah',\n",
       " '.',\n",
       " 'Had',\n",
       " 'that',\n",
       " 'dream',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Then',\n",
       " 'I',\n",
       " 'look',\n",
       " 'down',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'realize',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'phone',\n",
       " '...',\n",
       " 'there',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Instead',\n",
       " 'of',\n",
       " '...',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'want',\n",
       " 'to',\n",
       " 'be',\n",
       " 'single',\n",
       " ',',\n",
       " 'okay',\n",
       " '?',\n",
       " 'I',\n",
       " 'just',\n",
       " '...',\n",
       " 'I',\n",
       " 'just',\n",
       " '...',\n",
       " 'I',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'married',\n",
       " 'again',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'And',\n",
       " 'I',\n",
       " 'just',\n",
       " 'want',\n",
       " 'a',\n",
       " 'million',\n",
       " 'dollars',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " 'my',\n",
       " 'God',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'know',\n",
       " ',',\n",
       " 'I',\n",
       " 'know',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'such',\n",
       " 'an',\n",
       " 'idiot',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " 'my',\n",
       " 'God',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'know',\n",
       " ',',\n",
       " 'I',\n",
       " 'know',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'such',\n",
       " 'an',\n",
       " 'idiot',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'I',\n",
       " 'should',\n",
       " 'have',\n",
       " 'caught',\n",
       " 'on',\n",
       " 'when',\n",
       " 'she',\n",
       " 'started',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'dentist',\n",
       " 'four',\n",
       " 'and',\n",
       " 'five',\n",
       " 'times',\n",
       " 'a',\n",
       " 'week',\n",
       " '.',\n",
       " 'I',\n",
       " 'mean',\n",
       " ',',\n",
       " 'how',\n",
       " 'clean',\n",
       " 'can',\n",
       " 'teeth',\n",
       " 'get',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'My',\n",
       " 'brother',\n",
       " 'going',\n",
       " 'through',\n",
       " 'that',\n",
       " 'right',\n",
       " 'now',\n",
       " ',',\n",
       " 'he',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'mess',\n",
       " '.',\n",
       " 'How',\n",
       " 'did',\n",
       " 'you',\n",
       " 'get',\n",
       " 'through',\n",
       " 'it',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Well',\n",
       " ',',\n",
       " 'you',\n",
       " 'might',\n",
       " 'try',\n",
       " 'accidentally',\n",
       " 'breaking',\n",
       " 'something',\n",
       " 'valuable',\n",
       " 'of',\n",
       " 'hers',\n",
       " ',',\n",
       " 'say',\n",
       " 'her',\n",
       " '...',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " 'my',\n",
       " 'God',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'know',\n",
       " ',',\n",
       " 'I',\n",
       " 'know',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'such',\n",
       " 'an',\n",
       " 'idiot',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'I',\n",
       " 'should',\n",
       " 'have',\n",
       " 'caught',\n",
       " 'on',\n",
       " 'when',\n",
       " 'she',\n",
       " 'started',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'dentist',\n",
       " 'four',\n",
       " 'and',\n",
       " 'five',\n",
       " 'times',\n",
       " 'a',\n",
       " 'week',\n",
       " '.',\n",
       " 'I',\n",
       " 'mean',\n",
       " ',',\n",
       " 'how',\n",
       " 'clean',\n",
       " 'can',\n",
       " 'teeth',\n",
       " 'get',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'My',\n",
       " 'brother',\n",
       " 'going',\n",
       " 'through',\n",
       " 'that',\n",
       " 'right',\n",
       " 'now',\n",
       " ',',\n",
       " 'he',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'mess',\n",
       " '.',\n",
       " 'How',\n",
       " 'did',\n",
       " 'you',\n",
       " 'get',\n",
       " 'through',\n",
       " 'it',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Well',\n",
       " ',',\n",
       " 'you',\n",
       " 'might',\n",
       " 'try',\n",
       " 'accidentally',\n",
       " 'breaking',\n",
       " 'something',\n",
       " 'valuable',\n",
       " 'of',\n",
       " 'hers',\n",
       " ',',\n",
       " 'say',\n",
       " 'her',\n",
       " '...',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'leg',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " 'my',\n",
       " 'God',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'know',\n",
       " ',',\n",
       " 'I',\n",
       " 'know',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'such',\n",
       " 'an',\n",
       " 'idiot',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'I',\n",
       " 'should',\n",
       " 'have',\n",
       " 'caught',\n",
       " 'on',\n",
       " 'when',\n",
       " 'she',\n",
       " 'started',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'dentist',\n",
       " 'four',\n",
       " 'and',\n",
       " 'five',\n",
       " 'times',\n",
       " 'a',\n",
       " 'week',\n",
       " '.',\n",
       " 'I',\n",
       " 'mean',\n",
       " ',',\n",
       " 'how',\n",
       " 'clean',\n",
       " 'can',\n",
       " 'teeth',\n",
       " 'get',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'My',\n",
       " 'brother',\n",
       " 'going',\n",
       " 'through',\n",
       " 'that',\n",
       " 'right',\n",
       " 'now',\n",
       " ',',\n",
       " 'he',\n",
       " 'is',\n",
       " 'such',\n",
       " 'a',\n",
       " 'mess',\n",
       " '.',\n",
       " 'How',\n",
       " 'did',\n",
       " 'you',\n",
       " 'get',\n",
       " 'through',\n",
       " 'it',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Well',\n",
       " ',',\n",
       " 'you',\n",
       " 'might',\n",
       " 'try',\n",
       " 'accidentally',\n",
       " 'breaking',\n",
       " 'something',\n",
       " 'valuable',\n",
       " 'of',\n",
       " 'hers',\n",
       " ',',\n",
       " 'say',\n",
       " 'her',\n",
       " '...',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'leg',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'That',\n",
       " 'is',\n",
       " 'one',\n",
       " 'way',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Me',\n",
       " ',',\n",
       " 'I',\n",
       " '...',\n",
       " 'I',\n",
       " 'went',\n",
       " 'for',\n",
       " 'the',\n",
       " 'watch',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " ',',\n",
       " 'look',\n",
       " ',',\n",
       " 'wish',\n",
       " 'me',\n",
       " 'luck',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'What',\n",
       " 'for',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'You',\n",
       " 'know',\n",
       " 'you',\n",
       " 'probably',\n",
       " 'did',\n",
       " 'not',\n",
       " 'know',\n",
       " 'this',\n",
       " ',',\n",
       " 'but',\n",
       " 'back',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " ',',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " ',',\n",
       " 'um',\n",
       " ',',\n",
       " 'major',\n",
       " 'crush',\n",
       " 'on',\n",
       " 'you',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'knew',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'You',\n",
       " 'know',\n",
       " 'you',\n",
       " 'probably',\n",
       " 'did',\n",
       " 'not',\n",
       " 'know',\n",
       " 'this',\n",
       " ',',\n",
       " 'but',\n",
       " 'back',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " ',',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " ',',\n",
       " 'um',\n",
       " ',',\n",
       " 'major',\n",
       " 'crush',\n",
       " 'on',\n",
       " 'you',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'knew',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'You',\n",
       " 'did',\n",
       " '!',\n",
       " 'Oh',\n",
       " '...',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'always',\n",
       " 'figured',\n",
       " 'you',\n",
       " 'just',\n",
       " 'thought',\n",
       " 'I',\n",
       " 'was',\n",
       " 'Monica',\n",
       " 'geeky',\n",
       " 'older',\n",
       " 'brother',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'did',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'You',\n",
       " 'know',\n",
       " 'you',\n",
       " 'probably',\n",
       " 'did',\n",
       " 'not',\n",
       " 'know',\n",
       " 'this',\n",
       " ',',\n",
       " 'but',\n",
       " 'back',\n",
       " 'in',\n",
       " 'high',\n",
       " 'school',\n",
       " ',',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " ',',\n",
       " 'um',\n",
       " ',',\n",
       " 'major',\n",
       " 'crush',\n",
       " 'on',\n",
       " 'you',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'knew',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'You',\n",
       " 'did',\n",
       " '!',\n",
       " 'Oh',\n",
       " '...',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'always',\n",
       " 'figured',\n",
       " 'you',\n",
       " 'just',\n",
       " 'thought',\n",
       " 'I',\n",
       " 'was',\n",
       " 'Monica',\n",
       " 'geeky',\n",
       " 'older',\n",
       " 'brother',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'did',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Oh',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " '...',\n",
       " 'and',\n",
       " 'try',\n",
       " 'not',\n",
       " 'to',\n",
       " 'let',\n",
       " 'my',\n",
       " 'intense',\n",
       " 'vulnerability',\n",
       " 'become',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'a',\n",
       " 'factor',\n",
       " 'here',\n",
       " '...',\n",
       " 'but',\n",
       " 'do',\n",
       " 'you',\n",
       " 'think',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'okay',\n",
       " 'if',\n",
       " 'I',\n",
       " 'asked',\n",
       " 'you',\n",
       " 'out',\n",
       " '?',\n",
       " 'Sometime',\n",
       " '?',\n",
       " 'Maybe',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'What',\n",
       " 'that',\n",
       " 'Rachel',\n",
       " 'did',\n",
       " 'to',\n",
       " 'her',\n",
       " 'life',\n",
       " '...',\n",
       " 'We',\n",
       " 'ran',\n",
       " 'into',\n",
       " 'her',\n",
       " 'parents',\n",
       " 'at',\n",
       " 'the',\n",
       " 'club',\n",
       " ',',\n",
       " 'they',\n",
       " 'were',\n",
       " 'not',\n",
       " 'playing',\n",
       " 'very',\n",
       " 'well',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'What',\n",
       " 'that',\n",
       " 'Rachel',\n",
       " 'did',\n",
       " 'to',\n",
       " 'her',\n",
       " 'life',\n",
       " '...',\n",
       " 'We',\n",
       " 'ran',\n",
       " 'into',\n",
       " 'her',\n",
       " 'parents',\n",
       " 'at',\n",
       " 'the',\n",
       " 'club',\n",
       " ',',\n",
       " 'they',\n",
       " 'were',\n",
       " 'not',\n",
       " 'playing',\n",
       " 'very',\n",
       " 'well',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'I',\n",
       " 'am',\n",
       " 'not',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'what',\n",
       " 'they',\n",
       " 'spent',\n",
       " 'on',\n",
       " 'that',\n",
       " 'wedding',\n",
       " '...',\n",
       " 'but',\n",
       " 'forty',\n",
       " 'thousand',\n",
       " 'dollars',\n",
       " 'is',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'money',\n",
       " '!',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Well',\n",
       " ',',\n",
       " 'at',\n",
       " 'least',\n",
       " 'she',\n",
       " 'had',\n",
       " 'the',\n",
       " 'chance',\n",
       " 'to',\n",
       " 'leave',\n",
       " 'a',\n",
       " 'man',\n",
       " 'at',\n",
       " 'the',\n",
       " 'altar',\n",
       " '...',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Sorry',\n",
       " 'I',\n",
       " 'am',\n",
       " 'late',\n",
       " ',',\n",
       " 'I',\n",
       " 'was',\n",
       " 'stuck',\n",
       " 'at',\n",
       " 'work',\n",
       " '.',\n",
       " 'There',\n",
       " 'was',\n",
       " 'this',\n",
       " 'big',\n",
       " 'dinosaur',\n",
       " '...',\n",
       " 'thing',\n",
       " '...',\n",
       " 'anyway',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Hi',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Ross',\n",
       " ',',\n",
       " 'you',\n",
       " 'remember',\n",
       " 'Susan',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'How',\n",
       " 'could',\n",
       " 'I',\n",
       " 'forget',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Ross',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Hello',\n",
       " ',',\n",
       " 'Susan',\n",
       " '.',\n",
       " 'Good',\n",
       " 'shake',\n",
       " '.',\n",
       " 'Good',\n",
       " 'shake',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'So',\n",
       " ',',\n",
       " 'uh',\n",
       " ',',\n",
       " 'we',\n",
       " 'are',\n",
       " 'just',\n",
       " 'waiting',\n",
       " 'for',\n",
       " '...',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Dr',\n",
       " '.',\n",
       " 'Oberman',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " '...',\n",
       " 'Dr',\n",
       " '.',\n",
       " 'Oberman',\n",
       " '.',\n",
       " 'Okay',\n",
       " '.',\n",
       " 'And',\n",
       " 'is',\n",
       " 'he',\n",
       " '...',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'She',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'she',\n",
       " ',',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " 'she',\n",
       " '...',\n",
       " 'uh',\n",
       " '...',\n",
       " 'familiar',\n",
       " 'with',\n",
       " 'our',\n",
       " '...',\n",
       " 'special',\n",
       " 'situation',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Sorry',\n",
       " 'I',\n",
       " 'am',\n",
       " 'late',\n",
       " ',',\n",
       " 'I',\n",
       " 'was',\n",
       " 'stuck',\n",
       " 'at',\n",
       " 'work',\n",
       " '.',\n",
       " 'There',\n",
       " 'was',\n",
       " 'this',\n",
       " 'big',\n",
       " 'dinosaur',\n",
       " '...',\n",
       " 'thing',\n",
       " '...',\n",
       " 'anyway',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Hi',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Ross',\n",
       " ',',\n",
       " 'you',\n",
       " 'remember',\n",
       " 'Susan',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'How',\n",
       " 'could',\n",
       " 'I',\n",
       " 'forget',\n",
       " '?',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Ross',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'Hello',\n",
       " ',',\n",
       " 'Susan',\n",
       " '.',\n",
       " 'Good',\n",
       " 'shake',\n",
       " '.',\n",
       " 'Good',\n",
       " 'shake',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<SOS>',\n",
       " 'So',\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78d58e91-3b2c-4e42-8fd6-184d8eae1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model2 = gensim.models.Word2Vec(tokenized_data, min_count = 1, vector_size = 100,\n",
    "                                             window = 5, sg = 1)\n",
    "word_vectors = model2.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae2a98ef-4c1b-46f9-b016-cd29346a8790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09,  0.1 ,  0.  , -0.05,  0.22, -0.18, -0.05,  0.31, -0.28,\n",
       "       -0.26, -0.09, -0.25,  0.2 ,  0.08,  0.14, -0.1 , -0.09,  0.03,\n",
       "       -0.05, -0.3 ,  0.38, -0.13, -0.06,  0.08, -0.19, -0.02, -0.07,\n",
       "        0.11, -0.27,  0.02, -0.05,  0.15,  0.07, -0.18, -0.05,  0.11,\n",
       "       -0.  ,  0.07,  0.16, -0.08, -0.01, -0.33,  0.04,  0.16,  0.21,\n",
       "       -0.07, -0.17, -0.15,  0.22,  0.04,  0.12, -0.15,  0.1 , -0.15,\n",
       "       -0.  , -0.07, -0.  ,  0.08, -0.09, -0.04,  0.03, -0.02,  0.24,\n",
       "        0.05, -0.26,  0.25,  0.1 ,  0.25, -0.26,  0.  , -0.22,  0.17,\n",
       "        0.39,  0.06,  0.23,  0.26,  0.21,  0.1 , -0.21,  0.04, -0.16,\n",
       "       -0.15, -0.05,  0.17, -0.2 , -0.04,  0.15,  0.21,  0.42,  0.05,\n",
       "        0.07,  0.23, -0.2 , -0.15,  0.08,  0.14,  0.1 , -0.06,  0.2 ,\n",
       "        0.  ], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(word_vectors['teacher'], decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b04d1ed3-b8eb-4017-a1a8-ca9281a7aece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8147575"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('syphilis', topn = 10)\n",
    "model2.wv.similarity('teacher', 'syphilis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a18ce2ad-caf9-4bed-96ce-67c0152f5186",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w', encoding=\"utf-8\") as vf:\n",
    "    vf.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9210eaa1-e8e5-4451-b997-7a53d3514277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'utterance_ID': 1,\n",
       "  'text': 'Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .',\n",
       "  'speaker': 'Chandler',\n",
       "  'emotion': 'neutral'},\n",
       " {'utterance_ID': 2,\n",
       "  'text': 'Oh , yeah . Had that dream .',\n",
       "  'speaker': 'All',\n",
       "  'emotion': 'neutral'},\n",
       " {'utterance_ID': 3,\n",
       "  'text': 'Then I look down , and I realize there is a phone ... there .',\n",
       "  'speaker': 'Chandler',\n",
       "  'emotion': 'surprise'},\n",
       " {'utterance_ID': 4,\n",
       "  'text': 'Instead of ... ?',\n",
       "  'speaker': 'Joey',\n",
       "  'emotion': 'surprise'},\n",
       " {'utterance_ID': 5,\n",
       "  'text': 'That is right .',\n",
       "  'speaker': 'Chandler',\n",
       "  'emotion': 'anger'},\n",
       " {'utterance_ID': 6,\n",
       "  'text': 'Never had that dream .',\n",
       "  'speaker': 'Joey',\n",
       "  'emotion': 'neutral'},\n",
       " {'utterance_ID': 7,\n",
       "  'text': 'No .',\n",
       "  'speaker': 'Phoebe',\n",
       "  'emotion': 'neutral'},\n",
       " {'utterance_ID': 8,\n",
       "  'text': 'All of a sudden , the phone starts to ring .',\n",
       "  'speaker': 'Chandler',\n",
       "  'emotion': 'neutral'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sdata[0]['conversation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842ec80b-4b1f-47f4-ba27-0be264c49428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchcrf import CRF\n",
    "num_tags = 5  # number of tags is 5\n",
    "model = CRF(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40eef8d3-4f33-4788-9c0f-3158cfb6c5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0268, -0.7828,  1.2775, -1.1456, -0.5920],\n",
      "        [ 1.3204, -0.7955,  2.2738,  2.1359,  0.9672]])\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 3  # maximum sequence length in a batch\n",
    "batch_size = 2  # number of samples in the batch\n",
    "emissions = torch.randn(seq_length, batch_size, num_tags)\n",
    "\n",
    "tags = torch.tensor([[0, 1], [2, 4], [3, 1] ], dtype=torch.long)  # (seq_length, batch_size)\n",
    "print(emissions[0])\n",
    "print(tags[0])\n",
    "#model(emissions, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aaefa34-b759-4b14-a387-04420b081056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.7970, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tensor([[1, 1], [1, 1], [1, 0] ], dtype=torch.uint8)\n",
    "model(emissions, tags, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f8176e-d06c-4f9e-813b-b4ba2716a56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2, 4], [0, 4, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(emissions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
