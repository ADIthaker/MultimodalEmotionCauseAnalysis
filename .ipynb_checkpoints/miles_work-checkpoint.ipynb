{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "baab05c3-becf-4de7-9fc2-83a0d1014175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import evaluate as ev\n",
    "import re\n",
    "import semEval as sem\n",
    "import torch\n",
    "import random\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f73f000-0302-4284-9135-3f32d72a135a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emotion_utterance_ID': 'dia1utt3',\n",
       " 'emotion': 'surprise',\n",
       " 'conversation': [{'utterance_ID': 1,\n",
       "   'text': 'Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .',\n",
       "   'speaker': 'Chandler',\n",
       "   'emotion': 'neutral'},\n",
       "  {'utterance_ID': 2,\n",
       "   'text': 'Oh , yeah . Had that dream .',\n",
       "   'speaker': 'All',\n",
       "   'emotion': 'neutral'},\n",
       "  {'utterance_ID': 3,\n",
       "   'text': 'Then I look down , and I realize there is a phone ... there .',\n",
       "   'speaker': 'Chandler',\n",
       "   'emotion': 'surprise'},\n",
       "  {'utterance_ID': 4,\n",
       "   'text': 'Instead of ... ?',\n",
       "   'speaker': 'Joey',\n",
       "   'emotion': 'surprise'},\n",
       "  {'utterance_ID': 5,\n",
       "   'text': 'That is right .',\n",
       "   'speaker': 'Chandler',\n",
       "   'emotion': 'anger'},\n",
       "  {'utterance_ID': 6,\n",
       "   'text': 'Never had that dream .',\n",
       "   'speaker': 'Joey',\n",
       "   'emotion': 'neutral'},\n",
       "  {'utterance_ID': 7,\n",
       "   'text': 'No .',\n",
       "   'speaker': 'Phoebe',\n",
       "   'emotion': 'neutral'},\n",
       "  {'utterance_ID': 8,\n",
       "   'text': 'All of a sudden , the phone starts to ring .',\n",
       "   'speaker': 'Chandler',\n",
       "   'emotion': 'neutral'}],\n",
       " 'cause_spans': ['1_I realize I am totally naked .',\n",
       "  '3_Then I look down , and I realize there is a phone ... there .']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data with semEval function\n",
    "data = ev.get_json_data('data/Subtask_1_1_train.json')\n",
    "data[:2]\n",
    "# converts the list of dictionaries into one large dictionary where the main callable key is the emotion_utterance_ID\n",
    "# that way we can call a specific emotion_utterance_ID dicretly from this dict it reduces the data one level\n",
    "dicts = ev.convert_list_to_dict(data, main_key = 'emotion_utterance_ID')\n",
    "dicts['dia1utt3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b563b0a7-1c4b-43b0-afd8-35eb80e275cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cpu\")\n",
    "# TODO: Uncomment the below line if you see True in the print statement\n",
    "# device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "263262ac-de77-42c5-b5b5-ceb09bac8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data  of {'pos_cause': , 'target': , 'label': , }\n",
    "nli_data_pairs = []\n",
    "for d in data:\n",
    "    # get the target text\n",
    "    convo_id, target_utt_id = sem.get_target_conv_utt_ids(d['emotion_utterance_ID'])\n",
    "    target_text = d['conversation'][int(target_utt_id) - 1]['text']\n",
    "    cause_spans_ids = sem.get_cause_span_ids(d['cause_spans'])\n",
    "    # for every conversation\n",
    "    for utterance in d['conversation']:\n",
    "        label = 1 if utterance['utterance_ID'] in cause_spans_ids else 0\n",
    "        nli_data_pairs.append({'pos_cause': utterance['text'],'target': target_text,'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2916d-7d9a-45c1-b0a2-8db91cc74aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test val\n",
    "random.seed(42)\n",
    "random.shuffle(nli_data_pairs)\n",
    "train, val, test = sem.get_train_val_test(nli_data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9dea19cc-a826-40dc-9cb1-508bb6311698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5999933315550814 0.1038436602760026\n",
      "0.19999777718502712 0.10575159766601834\n",
      "0.20000889125989152 0.09707712825072239\n",
      "0.1028718769449631\n"
     ]
    }
   ],
   "source": [
    "print(len(train) / len(nli_data_pairs), sem.count_labels(train) / len(train))\n",
    "print(len(val) / len(nli_data_pairs), sem.count_labels(val)/ len(val))\n",
    "print(len(test) / len(nli_data_pairs), sem.count_labels(test) / len(test))\n",
    "print( sem.count_labels(nli_data_pairs) / len(nli_data_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef229d4c-ca86-416b-b4b2-e47eaf650f73",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e330107b63b040b09efccbd5c586958c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\menam\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\menam\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49737d2a170e4d6ab944433ce0208d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1996,  2034, 18458,   102,  2023,  2003,  2034,\n",
      "         10744,   102,     0],\n",
      "        [  101,  2023,  2003,  1996,  2117, 18458,   102,  2023,  2003,  1996,\n",
      "          2117, 10744,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] this is the first premise [SEP] this is first hypothesis [SEP] [PAD]',\n",
       " '[CLS] this is the second premise [SEP] this is the second hypothesis [SEP]']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = sem.BatchTokenizer()\n",
    "x = tokenizer(*[[\"this is the first premise\", \"This is the second premise\"], [\"This is first hypothesis\", \"This is the second hypothesis\"]])\n",
    "print(x)\n",
    "tokenizer.hf_tokenizer.batch_decode(x[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8615b2-6272-40b3-9838-0afefc57e73b",
   "metadata": {},
   "source": [
    "# We can batch the train, validation, and test data, and then run it through the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f313abe-f366-4fa0-8e5f-966f108a4728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d15cb669-8f2f-4a76-ac47-fd999cfb820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_causes, train_targets, train_labels = sem.generate_pairwise_input(train)\n",
    "validation_causes, validation_targets, validation_labels = sem.generate_pairwise_input(val)\n",
    "test_causes, test_targets, test_labels = sem.generate_pairwise_input(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37fd8a2f-83c0-4a55-9db4-05244a76e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch and tokenize the train causes and targets\n",
    "batch_size = 16\n",
    "\n",
    "# Notice that since we use huggingface, we tokenize and\n",
    "# encode in all at once!\n",
    "tokenizer = sem.BatchTokenizer()\n",
    "train_input_batches = [b for b in sem.chunk_multi(train_causes, train_targets, batch_size)]\n",
    "#print(train_input_batches)\n",
    "# Tokenize + encode\n",
    "train_input_batches = [tokenizer(*batch) for batch in train_input_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68ce8837-9683-4bfd-8ddd-2b3afd07cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch the labels\n",
    "train_label_batches = [b for b in chunk(train_labels, batch_size)]\n",
    "train_label_batches = [sem.encode_labels(batch) for batch in train_label_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9a4ee66d-9e1e-4bbb-bdf8-3b19363a453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_size: int, hidden_size: int, model_name='prajjwal1/bert-small'):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize BERT, which we use instead of a single embedding layer.\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "        # TODO [OPTIONAL]: Updating all BERT parameters can be slow and memory intensive.\n",
    "        # Freeze them if training is too slow. Notice that the learning\n",
    "        # rate should probably be smaller in this case.\n",
    "        # Uncommenting out the below 2 lines means only our classification layer will be updated.\n",
    "\n",
    "        # for param in self.bert.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "\n",
    "        # TODO: Add an extra hidden layer in the classifier, projecting\n",
    "        #      from the BERT hidden dimension to hidden size. Hint: torch.nn.Linear(), should we have a bias term?\n",
    "        # torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "\n",
    "        self.hidden_layer = torch.nn.Linear(self.bert_hidden_dimension, self.hidden_size)\n",
    "\n",
    "        # TODO: Add a relu nonlinearity to be used in the forward method\n",
    "        #      https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "        # torch.nn.RelU returns a function that uses the ReLU logic\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def encode_text(self,symbols):\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols BERT.\n",
    "            Then, get CLS represenation.\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: CLS token embedding\n",
    "        \"\"\"\n",
    "        # First we get the contextualized embedding for each input symbol\n",
    "        # We no longer need an LSTM, since BERT encodes context and\n",
    "        # gives us a single vector describing the sequence in the form of the [CLS] token.\n",
    "        encoded_sequence = self.bert(**symbols)\n",
    "        # TODO: Get the [CLS] token\n",
    "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "        #      and check the returns for the forward method.\n",
    "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
    "        # print(encoded_sequence.last_hidden_state.shape)\n",
    "        # Return only the first token's embedding from the last_hidden_state. Hint: using list slices\n",
    "        return encoded_sequence.last_hidden_state[:,0,:]\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,symbols):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.hidden_layer(encoded_sents)\n",
    "        output = self.relu(output)\n",
    "        output = self.classifier(output)\n",
    "        return self.log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bebaa47b-4e37-4518-867a-98cd6c8baf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For making predictions at test time\n",
    "def predict(model, sents):\n",
    "    #logits = model(sents)\n",
    "    #return list(torch.argmax(logits, axis=1).squeeze().numpy()) # changed the axis to be 1\n",
    "\n",
    "    logits = model(sents.to(device))\n",
    "    return list(torch.Tensor.cpu(torch.argmax(logits, axis=1).squeeze()).numpy()) # change the axis to be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8267e9f7-7cdf-4e58-a8cd-75e2870f1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_sents,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for features, labels in tqdm(batches):\n",
    "\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            device = torch.device('cuda:0')\n",
    "            #print(features.to(device))\n",
    "            preds = model(features.to(device)).squeeze(1)\n",
    "            loss = loss_func(preds, labels.to(device))\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            #pred = predict(model, sents).cpu()\n",
    "\n",
    "            pred = predict(model, sents)\n",
    "\n",
    "            all_preds.extend(pred)\n",
    "            all_labels.extend(list(labels.cpu().numpy()))\n",
    "\n",
    "        #dev_f1 = macro_f1_score(all_preds, all_labels)\n",
    "        dev_f1 = macro_f1(all_preds, all_labels, possible_labels =  list(set(all_labels))  )\n",
    "\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aa603828-8235-4eed-9071-536ef617cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m validation_batch_labels \u001b[38;5;241m=\u001b[39m [b \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m chunk(validation_labels, batch_size)]\n\u001b[0;32m     24\u001b[0m validation_batch_labels \u001b[38;5;241m=\u001b[39m [encode_labels(batch) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m validation_batch_labels]\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_input_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_label_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_input_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_batch_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[81], line 16\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(num_epochs, train_features, train_labels, dev_sents, dev_labels, optimizer, model)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     15\u001b[0m     losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m features, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(batches):\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Empty the dynamic computation graph\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     20\u001b[0m         device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# You can increase epochs if need be\n",
    "epochs = 20\n",
    "\n",
    "# TODO: Find a good learning rate and hidden size\n",
    "LR = 0.01\n",
    "hidden_size = 10\n",
    "\n",
    "possible_labels = set(train_labels)\n",
    "# we build this.\n",
    "model = NLIClassifier(output_size=len(possible_labels), hidden_size=hidden_size)\n",
    "# device tells us which GPU to use?\n",
    "model.to(device)\n",
    "# This is the optimizer from torch. we pass it parameters and the learnig rate\n",
    "# how does adamW optimize the weights?\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "# this is a class we have\n",
    "batch_tokenizer = sem.BatchTokenizer()\n",
    "# create batches of validations prem and hypothesis\n",
    "validation_input_batches = [b for b in chunk_multi(validation_causes, validation_targets, batch_size)]\n",
    "\n",
    "# Tokenize + encode\n",
    "validation_input_batches = [batch_tokenizer(*batch) for batch in validation_input_batches]\n",
    "validation_batch_labels = [b for b in chunk(validation_labels, batch_size)]\n",
    "validation_batch_labels = [encode_labels(batch) for batch in validation_batch_labels]\n",
    "\n",
    "training_loop(\n",
    "    epochs,\n",
    "    train_input_batches,\n",
    "    train_label_batches,\n",
    "    validation_input_batches,\n",
    "    validation_batch_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355523c-cabd-40d4-8feb-29cd774f46a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e15c2-01ad-4d0e-84f1-97c067f99ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e55d728-9570-45c3-9ea8-4f9b813d0554",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversation_id: 1\n",
      "['3_surprise', '1_21_27']\n",
      "['3_surprise', '3_0_14']\n",
      "\n",
      "conversation_id: 1\n",
      "['4_surprise', '1_21_27']\n",
      "['4_surprise', '3_0_14']\n",
      "['4_surprise', '4_0_2']\n",
      "\n",
      "conversation_id: 1\n",
      "['5_anger', '1_21_27']\n",
      "['5_anger', '3_0_14']\n",
      "['5_anger', '4_0_2']\n",
      "\n",
      "conversation_id: 2\n",
      "['1_sadness', '1_0_6']\n",
      "\n",
      "conversation_id: 2\n",
      "['3_surprise', '3_0_2']\n",
      "\n",
      "conversation_id: 3\n",
      "['3_sadness', '3_2_20']\n",
      "\n",
      "conversation_id: 3\n",
      "['6_surprise', '4_14_20']\n",
      "['6_surprise', '5_2_14']\n",
      "['6_surprise', '6_0_1']\n",
      "\n",
      "conversation_id: 3\n",
      "['7_joy', '5_2_14']\n",
      "['7_joy', '6_0_1']\n",
      "['7_joy', '7_0_4']\n",
      "\n",
      "conversation_id: 3\n",
      "['9_surprise', '5_2_10']\n",
      "['9_surprise', '8_4_9']\n",
      "['9_surprise', '9_0_5']\n",
      "\n",
      "conversation_id: 5\n",
      "['1_joy', '3_0_10']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the keys of the data dictionary\n",
    "dia_utts = list(dicts.keys())\n",
    "\n",
    "# iterate over the keys of the data dictionary\n",
    "\n",
    "\n",
    "for dia_utt in dia_utts[:10]:\n",
    "    # get what conversation we are looking at and the targeted utterance from dia_utt val, ex: 'dia1utt3' \n",
    "    convo_id, target_utt_id = sem.get_target_conv_utt_ids(dia_utt)\n",
    "   \n",
    "    print('conversation_id: {}'.format(convo_id))\n",
    "    \n",
    "    for span in dicts[dia_utt]['cause_spans']:\n",
    "        # create a dictionary of the conversation where the utterance is the key\n",
    "        # I am not sure I need to do this, but it make the code cleaner? It migh be slower or it might be faster than not\n",
    "        conversation = ev.convert_list_to_dict(dicts[dia_utt]['conversation'], main_key = 'utterance_ID')\n",
    "\n",
    "        # pull out the number from the utterances so that we have index that is relating to \n",
    "        # the original conversation\n",
    "        span_as_list = span.split('_')\n",
    "        utterance_ID = span_as_list[0]\n",
    "        \n",
    "        # get the full texts from the conversation\n",
    "        text = conversation[int(utterance_ID)]['text']\n",
    "        # return or print the begining and ending index of the slice in the utterance that belongs in the cause span\n",
    "        beg_end_idx = ev.get_span_position(span_as_list[1], text)\n",
    "        #print('{}_{}_{}'.format(utterance_ID,beg_end_idx[0], beg_end_idx[1]))\n",
    "        cause_pair = [target_utt_id + '_' + dicts[dia_utt]['emotion'],'{}_{}_{}'.format(utterance_ID,beg_end_idx[0], beg_end_idx[1] - 1)]\n",
    "        \n",
    "        print(cause_pair)\n",
    "    print()\n",
    "    \n",
    "\n",
    "#print(dicts['dia1utt3'])\n",
    "#ev.get_span_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c2694-9673-42f5-a223-33e8be67e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Init our model\n",
    "\n",
    "# 2. create optimzer\n",
    "\n",
    "# 3. Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cabee-0a0b-4666-a7e2-263869b22f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets work on how predictions are supposed to look\n",
    "# [conv_id, emo_utt_id, cau_utt_id, span_start_id, span_end_id, emotion_category]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215782a5-0bdf-4663-bf23-eb938e4896ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\menam\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.32.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\menam\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.1.1)\n",
      "Collecting torchvision (from sentence-transformers)\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/13/24/23cdf7e7dc33e5c01588c315f8424d31afa9edb05a80168f3d44f7178ff7/torchvision-0.16.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached torchvision-0.16.1-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\menam\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\menam\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.2)\n",
      "Requirement already satisfied: requests in c:\\users\\menam\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\menam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\menam\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\menam\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\menam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\menam\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Using cached torchvision-0.16.1-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: torchvision, sentence-transformers\n",
      "Successfully installed sentence-transformers-2.2.2 torchvision-0.16.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -U sentence-transformers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2df3dabb-55aa-487c-9d28-37c31448faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34d9401e-b49d-44b4-a259-29a602e81de8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e555d753daa9419e956dec59ee133d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edee186b5c1345c2aa3a3688883390a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366ba9d0de6842988e207b5ee1ee235e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/3.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c1bb89ba1a4939a840a9401c6063a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5573bbf7ebc245fc97b3a79b83080ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a20f439abc1467bb00a9b3a73cbc2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc211dc803ce4fcfa8c1bca38c2bacbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9259631ba8d24a298b5bf8bc4fed96e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4184ca75cf6f445c81a10fef10d1b2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a60972e3574f7db7b2bbe280b174d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85021dfc17d04df4b10539443613d9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d97db087ec449cc97e3a734b410bf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nNo module named 'torch.utils.checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1130\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.utils.checkpoint'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparaphrase-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Sentences we want to encode. Example:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m sentence \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis framework generates embeddings for each input sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\SentenceTransformer.py:95\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[0;32m     88\u001b[0m                             cache_dir\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[0;32m     89\u001b[0m                             library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m                             library_version\u001b[38;5;241m=\u001b[39m__version__,\n\u001b[0;32m     91\u001b[0m                             ignore_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflax_model.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrust_model.ot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     92\u001b[0m                             use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \u001b[38;5;66;03m#Load with AutoModel\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(model_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\SentenceTransformer.py:840\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module_config \u001b[38;5;129;01min\u001b[39;00m modules_config:\n\u001b[0;32m    839\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m import_from_string(module_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 840\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m     modules[module_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modules\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\models\\Transformer.py:137\u001b[0m, in \u001b[0;36mTransformer.load\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(sbert_config_path) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[0;32m    136\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fIn)\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\models\\Transformer.py:29\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[0;32m     28\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#No max_seq_length set. Try to infer from model\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\models\\Transformer.py:49\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_t5_model(model_name_or_path, config, cache_dir)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:515\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    512\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    513\u001b[0m     )\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 515\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    517\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:377\u001b[0m, in \u001b[0;36m_get_model_class\u001b[1;34m(config, model_mapping)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[1;32m--> 377\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:690\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[0;32m    689\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[0;32m    693\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:704\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:648\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, attr):\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    650\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1120\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1120\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1121\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1132\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1135\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nNo module named 'torch.utils.checkpoint'"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e97650-f542-4110-a6f8-33b8bb667a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a99894b7-bbfe-49eb-9bd5-6da85997bc24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': 0,\n",
       " 'anger': 1,\n",
       " 'disgust': 2,\n",
       " 'fear': 3,\n",
       " 'joy': 4,\n",
       " 'sadness': 5,\n",
       " 'surprise': 6}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am trying to understand how we need to format the prediction. it is extremely confusin\n",
    "emotion_idx = dict(zip(['neutral','anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise'], range(7)))\n",
    "emotion_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42a75f30-b3dd-47c4-b059-8d884c4fe998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dia1utt3', 'dia1utt4', 'dia1utt5', 'dia2utt1', 'dia2utt3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'emotion_utterance_ID': 'dia1utt3',\n",
       "  'emotion': 'surprise',\n",
       "  'conversation': [{'utterance_ID': 1,\n",
       "    'text': 'Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 2,\n",
       "    'text': 'Oh , yeah . Had that dream .',\n",
       "    'speaker': 'All',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 3,\n",
       "    'text': 'Then I look down , and I realize there is a phone ... there .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'surprise'},\n",
       "   {'utterance_ID': 4,\n",
       "    'text': 'Instead of ... ?',\n",
       "    'speaker': 'Joey',\n",
       "    'emotion': 'surprise'},\n",
       "   {'utterance_ID': 5,\n",
       "    'text': 'That is right .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'anger'},\n",
       "   {'utterance_ID': 6,\n",
       "    'text': 'Never had that dream .',\n",
       "    'speaker': 'Joey',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 7,\n",
       "    'text': 'No .',\n",
       "    'speaker': 'Phoebe',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 8,\n",
       "    'text': 'All of a sudden , the phone starts to ring .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'neutral'}],\n",
       "  'cause_spans': ['1_I realize I am totally naked .',\n",
       "   '3_Then I look down , and I realize there is a phone ... there .']},\n",
       " {'emotion_utterance_ID': 'dia1utt4',\n",
       "  'emotion': 'surprise',\n",
       "  'conversation': [{'utterance_ID': 1,\n",
       "    'text': 'Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 2,\n",
       "    'text': 'Oh , yeah . Had that dream .',\n",
       "    'speaker': 'All',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 3,\n",
       "    'text': 'Then I look down , and I realize there is a phone ... there .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'surprise'},\n",
       "   {'utterance_ID': 4,\n",
       "    'text': 'Instead of ... ?',\n",
       "    'speaker': 'Joey',\n",
       "    'emotion': 'surprise'},\n",
       "   {'utterance_ID': 5,\n",
       "    'text': 'That is right .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'anger'},\n",
       "   {'utterance_ID': 6,\n",
       "    'text': 'Never had that dream .',\n",
       "    'speaker': 'Joey',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 7,\n",
       "    'text': 'No .',\n",
       "    'speaker': 'Phoebe',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 8,\n",
       "    'text': 'All of a sudden , the phone starts to ring .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'neutral'}],\n",
       "  'cause_spans': ['1_I realize I am totally naked .',\n",
       "   '3_Then I look down , and I realize there is a phone ... there .',\n",
       "   '4_Instead of ...']},\n",
       " {'emotion_utterance_ID': 'dia1utt5',\n",
       "  'emotion': 'anger',\n",
       "  'conversation': [{'utterance_ID': 1,\n",
       "    'text': 'Alright , so I am back in high school , I am standing in the middle of the cafeteria , and I realize I am totally naked .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 2,\n",
       "    'text': 'Oh , yeah . Had that dream .',\n",
       "    'speaker': 'All',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 3,\n",
       "    'text': 'Then I look down , and I realize there is a phone ... there .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'surprise'},\n",
       "   {'utterance_ID': 4,\n",
       "    'text': 'Instead of ... ?',\n",
       "    'speaker': 'Joey',\n",
       "    'emotion': 'surprise'},\n",
       "   {'utterance_ID': 5,\n",
       "    'text': 'That is right .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'anger'},\n",
       "   {'utterance_ID': 6,\n",
       "    'text': 'Never had that dream .',\n",
       "    'speaker': 'Joey',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 7,\n",
       "    'text': 'No .',\n",
       "    'speaker': 'Phoebe',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 8,\n",
       "    'text': 'All of a sudden , the phone starts to ring .',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'neutral'}],\n",
       "  'cause_spans': ['1_I realize I am totally naked .',\n",
       "   '3_Then I look down , and I realize there is a phone ... there .',\n",
       "   '4_Instead of ...']},\n",
       " {'emotion_utterance_ID': 'dia2utt1',\n",
       "  'emotion': 'sadness',\n",
       "  'conversation': [{'utterance_ID': 1,\n",
       "    'text': 'I do not want to be single , okay ? I just ... I just ... I just wanna be married again !',\n",
       "    'speaker': 'Ross',\n",
       "    'emotion': 'sadness'},\n",
       "   {'utterance_ID': 2,\n",
       "    'text': 'And I just want a million dollars !',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 3,\n",
       "    'text': 'Rachel ? !',\n",
       "    'speaker': 'Monica',\n",
       "    'emotion': 'surprise'}],\n",
       "  'cause_spans': ['1_I do not want to be single']},\n",
       " {'emotion_utterance_ID': 'dia2utt3',\n",
       "  'emotion': 'surprise',\n",
       "  'conversation': [{'utterance_ID': 1,\n",
       "    'text': 'I do not want to be single , okay ? I just ... I just ... I just wanna be married again !',\n",
       "    'speaker': 'Ross',\n",
       "    'emotion': 'sadness'},\n",
       "   {'utterance_ID': 2,\n",
       "    'text': 'And I just want a million dollars !',\n",
       "    'speaker': 'Chandler',\n",
       "    'emotion': 'neutral'},\n",
       "   {'utterance_ID': 3,\n",
       "    'text': 'Rachel ? !',\n",
       "    'speaker': 'Monica',\n",
       "    'emotion': 'surprise'}],\n",
       "  'cause_spans': ['3_Rachel ? !']}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(dicts.keys())[:5])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832dda7-7a36-4d0b-a8a2-e94e997f812c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
