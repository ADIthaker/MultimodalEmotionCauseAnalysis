# -*- coding: utf-8 -*-
"""sentimentclass_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aOXgsYRd5HyTL7huVYCdtWqzbQN20Hct
"""

# -*- coding: utf-8 -*-
"""
@author: josef
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.data import random_split
from google.colab import drive
drive.mount('/content/drive')


if torch.cuda.is_available():
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU not available, using CPU")


num_labels = 3
batchSize = 32

embeddings = torch.load("/content/drive/MyDrive/embeddings_tensor.pt")
labels = torch.load("/content/drive/MyDrive/label_tensor2.pt")
mask = torch.load("/content/drive/MyDrive/masks_tensor2.pt")

print ("tensors loaded \n")
dataset = TensorDataset(embeddings,mask, labels)
train_size = int(0.80 * len(dataset))
dev_size = len(dataset) - train_size
train_dataset, dev_dataset = random_split(dataset, [train_size, dev_size])

train_loader = DataLoader(train_dataset, batch_size=batchSize, shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=batchSize, shuffle=False)

x = torch.tensor([[torch.nan, 1, 2, 1, 2, 3]])
print (x.nanmean())

from sklearn.metrics import f1_score
import numpy as np
import torch.optim as optim


class TransformerForTokenClassification(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_heads, num_layers, num_labels):
        super(TransformerForTokenClassification, self).__init__()

        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.output = nn.Linear(input_dim, num_labels)

    def forward(self, x, mask):
        x = self.transformer_encoder(x, src_key_padding_mask=mask)

        logits = self.output(x)

        return logits

model = TransformerForTokenClassification(
    input_dim=768,  # Dimension of BERT embeddings
    hidden_dim=256,
    num_heads=4,
    num_layers=2,
    num_labels=num_labels
)

model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)
torch.autograd.set_detect_anomaly(True)
num_epochs = 10

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    num_batches = 0
    for batch in train_loader:
        i,m, l = batch
        #m = m.transpose(0,1)
        i = i.transpose(0,1)
        l = l.transpose(0,1).to(torch.float)
        i =i.to(device)
        m =m.to(device)
        l =l.to(device)
        # Forward pass

        outputs = model(i, m)
        loss = criterion(outputs, l)



        '''
        outputs = outputs.view(-1, num_labels)  # Now: [batch_size * sequence_length, num_labels]
        non_nan_mask = ~torch.any(outputs.isnan(), dim=1)

        # Find indices of non-NaN elements
        non_nan_indices = torch.where(non_nan_mask)[0]
        outputs = outputs[non_nan_mask]
        l = l.view(-1)[non_nan_mask]
        loss = criterion(outputs, l)
        '''


        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        '''
        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        for name, param in model.named_parameters():
          if param.grad is not None:
            assert not torch.isnan(param.grad).any(), f"Gradient NaN detected in {name}"
        '''
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    avg_loss = total_loss / num_batches
    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')

    model.eval()  # Set the model to evaluation mode
    all_predictions = []
    all_labels = []
    with torch.no_grad():
        for batch in dev_loader:
            i, m, l = batch
            i = i.transpose(0,1)
            l = l.transpose(0,1).to(torch.float)
            i = i.to(device)
            m = m.to(device)
            l = l.to(device)

            outputs = model(i, m)
            predictions = torch.argmax(outputs, dim=-1).view(-1)
            labels = torch.argmax(l, dim=-1).view(-1)
            #labels = l.view(-1)

            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate F1 Score
    f1 = f1_score(all_labels, all_predictions, average='weighted')
    print(f"Epoch {epoch + 1}/{num_epochs}, F1 Score: {f1:.4f}")
    f1 = f1_score(all_labels, all_predictions, average='macro')
    print(f'Unweighted F1 Score: {f1:.4f}')